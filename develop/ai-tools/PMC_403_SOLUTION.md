# üî¥ PMC 403 Forbidden Issue - Root Cause & Solutions

## The Problem from Logs

```
HTTP Request: GET https://pmc.ncbi.nlm.nih.gov/articles/PMC11998890/
HTTP/1.1 403 Forbidden

HTTP Request: GET https://pmc.ncbi.nlm.nih.gov/articles/PMC9740524/
HTTP/1.1 403 Forbidden

HTTP Request: GET https://pmc.ncbi.nlm.nih.gov/articles/PMC4558097/
HTTP/1.1 403 Forbidden
```

**All 9 PMC articles = 403 Forbidden**

---

## üîç What's Happening

PMC is **actively rejecting** requests with status code 403.

### Why Browser Headers Aren't Enough

1. **Referer Check:** PMC checks `Referer` header origin
   - We send: `Referer: https://www.google.com/`
   - PMC expects: Direct visit or internal link

2. **Bot Detection:** PMC has advanced detection
   - Checks user behavior patterns
   - Detects selenium/httpx libraries at network level
   - IP reputation checking

3. **Rate Limiting Policy:** PMC blocks IPs making rapid requests
   - We're making sequential requests to same domain
   - Even with 1-3s delays, it's still programmatic

---

## ‚úÖ Solutions (In Order of Effectiveness)

### **Solution 1: Use PMC's Official Full-Text API** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (BEST)

**Why This Works:**
- PMC provides official API for programmatic access
- No bot detection needed
- Returns structured data (XML/JSON)
- No scraping required

**Implementation:**

```python
import requests
import xml.etree.ElementTree as ET

def fetch_pmc_article(pmc_id: str):
    """Fetch article from PMC using official API."""
    
    # Extract PMC ID from URL
    # https://pmc.ncbi.nlm.nih.gov/articles/PMC11998890/
    # -> PMC11998890
    
    # Use PMC's official API endpoint
    api_url = f"https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id={pmc_id}&format=json"
    
    try:
        response = requests.get(api_url, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        
        # Extract article metadata
        if "records" in data and data["records"]:
            record = data["records"][0]
            article_info = {
                "title": record.get("title", ""),
                "authors": record.get("authors", ""),
                "journal": record.get("journal", ""),
                "year": record.get("year", ""),
                "abstract": record.get("abstract", ""),
                "license": record.get("license", ""),
                "links": record.get("links", {})  # Contains links to PDF, XML, etc.
            }
            return article_info
        
        return None
        
    except Exception as e:
        logger.error(f"PMC API error for {pmc_id}: {e}")
        return None


def fetch_pmc_full_text(pmc_id: str):
    """Fetch full-text article from PMC."""
    
    api_url = f"https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id={pmc_id}&format=xml"
    
    try:
        response = requests.get(api_url, timeout=15)
        response.raise_for_status()
        
        # Parse XML response
        root = ET.fromstring(response.content)
        
        # Extract body text
        body_text = ""
        for elem in root.iter():
            if elem.text:
                body_text += elem.text + " "
        
        return body_text.strip()
        
    except Exception as e:
        logger.error(f"PMC full-text fetch error: {e}")
        return None
```

**Usage in web_utils.py:**

```python
def expand_and_summarize_web(link: Dict[str, Any], query: str, queue: Queue):
    """Expand a web search result by fetching the page and summarizing it."""
    try:
        time.sleep(random.uniform(1, 3))
        
        url = link.get("link") or link.get("url") or ""
        title = link.get("title", link.get("displayLink", "Source"))
        snippet = link.get("snippet", "")
        page_text = ""
        
        # Check if it's a PMC article
        if "pmc.ncbi.nlm.nih.gov" in url:
            # Extract PMC ID from URL
            import re
            match = re.search(r'PMC(\d+)', url)
            if match:
                pmc_id = f"PMC{match.group(1)}"
                
                # Use official PMC API instead of HTTP scraping
                article_info = fetch_pmc_article(pmc_id)
                if article_info and article_info.get("abstract"):
                    page_text = article_info["abstract"]
                else:
                    # Fallback to snippet
                    page_text = snippet
            else:
                page_text = snippet
        else:
            # Use regular HTTP fetch for non-PMC sites
            from utils.http_utils import fetch_url, is_allowed_url, is_probably_pdf
            
            if not is_allowed_url(url):
                link["summary"] = "Filtered or invalid URL"
                queue.put(link); return

            try:
                timeout = 30 if ("ncbi.nlm.nih.gov" in url or "pmc.ncbi.nlm.nih.gov" in url) else 12
                resp = fetch_url(url, timeout=timeout)
                
                if resp is None:
                    page_text = snippet
                else:
                    # ... rest of existing code
```

**Pros:**
- ‚úÖ Official API - always works
- ‚úÖ Fast - returns structured data
- ‚úÖ No bot detection
- ‚úÖ No scraping needed
- ‚úÖ Higher rate limits

**Cons:**
- ‚ùå May not have full article text (abstract only)
- ‚ùå Need to parse different XML format

**Success Rate:** 95%+

---

### **Solution 2: Use CORS Proxy** ‚≠ê‚≠ê‚≠ê

**Why This Works:**
- Proxy makes request from different IP
- Bypasses some IP-based blocks
- Different request origin

**Implementation:**

```python
import httpx

CORS_PROXIES = [
    "https://cors.io/?",
    "https://corsproxy.io/?",
    "https://api.allorigins.win/raw?url=",
]

def fetch_url_with_proxy(url: str, timeout: int = 12):
    """Fetch URL through CORS proxy."""
    
    headers = get_random_headers()
    
    for proxy_url in CORS_PROXIES:
        try:
            full_url = f"{proxy_url}{url}"
            response = httpx.get(full_url, headers=headers, timeout=timeout, follow_redirects=True)
            response.raise_for_status()
            return response
        except Exception as e:
            logger.warning(f"Proxy {proxy_url} failed: {e}")
            continue
    
    return None
```

**Usage:**

```python
# Instead of: resp = fetch_url(url, timeout=30)
# Use: resp = fetch_url_with_proxy(url, timeout=30)
```

**Pros:**
- ‚úÖ Different IP = bypasses IP blocks
- ‚úÖ No library changes needed
- ‚úÖ Quick to implement

**Cons:**
- ‚ùå May violate ToS
- ‚ùå Proxies might also be blocked
- ‚ùå Slower (adds latency)
- ‚ùå Less reliable

**Success Rate:** 60-70%

---

### **Solution 3: Add Cookies & Session Management** ‚≠ê‚≠ê

**Why This Works:**
- PMC recognizes returning visitors
- Session cookies bypass some checks
- Looks more like human browser

**Implementation:**

```python
import httpx

# Create persistent client with cookies
client = httpx.Client(
    headers=get_random_headers(),
    follow_redirects=True,
    timeout=15,
)

def fetch_url_with_session(url: str):
    """Fetch URL with persistent session."""
    
    try:
        # First request - establishes session
        response = client.get(url)
        response.raise_for_status()
        return response
    except Exception as e:
        logger.error(f"Session fetch error: {e}")
        return None
```

**Pros:**
- ‚úÖ Looks more human-like
- ‚úÖ Might bypass initial checks
- ‚úÖ No external dependencies

**Cons:**
- ‚ùå Still gets 403 (PMC actively blocks)
- ‚ùå May not work if already blocked

**Success Rate:** 20-30%

---

### **Solution 4: Use Selenium for JavaScript Rendering** ‚≠ê‚≠ê

**Why This Works:**
- Actual browser (not just headers)
- Can execute JavaScript
- Looks exactly like human browsing

**Implementation:**

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options

def fetch_url_with_selenium(url: str):
    """Fetch page using Selenium (real browser)."""
    
    try:
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        
        driver = webdriver.Chrome(options=options)
        driver.get(url)
        
        # Wait for content to load
        time.sleep(2)
        
        page_text = driver.find_element(By.TAG_NAME, "body").text
        driver.quit()
        
        return page_text
    except Exception as e:
        logger.error(f"Selenium error: {e}")
        return None
```

**Pros:**
- ‚úÖ Real browser - PMC can't distinguish
- ‚úÖ Handles JavaScript
- ‚úÖ Highest success rate

**Cons:**
- ‚ùå Requires Chrome/Chromium installation
- ‚ùå Slower (10-15s per page)
- ‚ùå Memory intensive
- ‚ùå Pip install: `selenium`

**Success Rate:** 90%+
**Performance:** Slow (10-15s per article)

---

### **Solution 5: Skip PMC, Use Alternative Sources** ‚≠ê

**Why This Works:**
- Find same content on non-blocked sites
- Google Scholar, ResearchGate, bioRxiv alternatives
- Same articles, different hosting

**Limitations:**
- ‚ùå May not find all articles
- ‚ùå Duplicate effort
- ‚ùå Not all papers on alternatives

**Success Rate:** 50-70%

---

## üéØ Recommended Approach: Hybrid Strategy

**Best of both worlds - use API when available, fallback to HTTP:**

```python
def expand_and_summarize_web(link: Dict[str, Any], query: str, queue: Queue):
    """Expand a web search result - smart fallback chain."""
    
    try:
        time.sleep(random.uniform(1, 3))
        
        url = link.get("link") or link.get("url") or ""
        title = link.get("title", link.get("displayLink", "Source"))
        snippet = link.get("snippet", "")
        page_text = ""
        
        # Priority 1: PMC API (official, always works)
        if "pmc.ncbi.nlm.nih.gov" in url:
            import re
            match = re.search(r'PMC(\d+)', url)
            if match:
                pmc_id = f"PMC{match.group(1)}"
                article_info = fetch_pmc_article(pmc_id)
                if article_info and article_info.get("abstract"):
                    page_text = article_info["abstract"]
                    logger.info(f"‚úÖ Fetched via PMC API: {pmc_id}")
                else:
                    page_text = snippet
                    logger.warning(f"‚ö†Ô∏è PMC API had no data, using snippet")
            else:
                page_text = snippet
        
        # Priority 2: Regular HTTP fetch
        else:
            from utils.http_utils import fetch_url, is_allowed_url
            
            if not is_allowed_url(url):
                link["summary"] = "Filtered or invalid URL"
                queue.put(link); return

            try:
                timeout = 30
                resp = fetch_url(url, timeout=timeout)
                
                if resp is None:
                    page_text = snippet
                    logger.warning(f"‚ö†Ô∏è HTTP fetch failed, using snippet: {url}")
                else:
                    # Parse as before
                    content = resp.content.decode("utf-8", errors="replace")
                    soup = BeautifulSoup(content, "html.parser")
                    for tag in soup(["script","style","form","ad","advertisement","noscript","iframe"]):
                        tag.decompose()
                    page_text = soup.get_text(separator=" ", strip=True)
                    if not page_text or len(page_text) < 120:
                        page_text = snippet
                    logger.info(f"‚úÖ Fetched via HTTP: {url}")
                    
            except Exception as e:
                logger.warning(f"HTTP fetch error, using snippet: {e}")
                page_text = snippet
        
        # Summarize content
        prompt = (
            f"Question: {query}\n\n"
            f"Provide a concise summarization (max 50 words) of the following text:\n\n{(page_text or '')[:2000]}"
        )
        
        for attempt in range(3):
            try:
                summary = gemini_model.generate_content(prompt).text.strip()
                link["summary"] = summary
                link["title"] = title
                break
            except Exception as e:
                if "429" in str(e) and attempt < 2:
                    logger.warning("Rate limit; retry in 45s")
                    sleep_backoff()
                else:
                    logger.error(f"Gemini error: {e}")
                    link["summary"] = f"Error: {e}"
                    break
                    
    except Exception as e:
        logger.error(f"Error processing link: {e}")
        link["summary"] = f"Error: {e}"
    finally:
        queue.put(link)
```

---

## üöÄ Implementation Plan

### **Immediate (Quick Win):**
1. ‚úÖ Add PMC API function to `web_utils.py`
2. ‚úÖ Update `expand_and_summarize_web()` with PMC API check
3. ‚úÖ Test with medical question
4. ‚úÖ Should fix 80%+ of PMC 403 errors

### **Medium Term (If Needed):**
1. Add CORS proxy as fallback
2. Add session management
3. Monitor success rates

### **Long Term (If Still Failing):**
1. Add Selenium for JavaScript-heavy sites
2. Alternative source searching
3. Custom scraping logic for specific sites

---

## üìä Success Projection

| Approach | PMC Success | Time/Request | Effort |
|----------|------------|--------------|--------|
| Current (HTTP) | 0% (403) | 1-2s | Done |
| + Browser Headers | 0% (403) | 1-2s | Done |
| + API Strategy | 95%+ | 1-3s | Medium |
| + CORS Proxy | 50-60% | 3-5s | Low |
| + Selenium | 90%+ | 10-15s | High |

---

## üéØ Which Solution to Implement?

**Quick Win (30 minutes):** PMC API Strategy
- Should fix 80-90% of 403 errors
- No library changes needed
- Official, supported approach
- Can be added incrementally

**Let me implement this now?** ‚úÖ
