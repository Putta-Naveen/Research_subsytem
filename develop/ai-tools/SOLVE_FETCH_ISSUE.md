# Solving "Failed to fetch page" Issue

## ðŸ”´ The Problem

```
Google CSE returns: https://pmc.ncbi.nlm.nih.gov/articles/PMC12240282/
                              â†“
Your code tries to fetch it
                              â†“
Result: "Failed to fetch page"
                              â†“
No content extracted for summarization
```

---

## ðŸ” Root Causes

### **1. Site Blocking Requests (Most Common)**
- **Problem:** PMC, PubMed, NIH sites block automated requests
- **Why:** They detect we're not a browser
- **Example:** 403 Forbidden, 429 Too Many Requests

### **2. Network/Timeout Issues**
- **Problem:** Page takes too long to load
- **Why:** Current timeout is 12 seconds (too short for some sites)
- **Example:** Connection timeout, read timeout

### **3. JavaScript-Heavy Content**
- **Problem:** Page loads content dynamically with JavaScript
- **Why:** BeautifulSoup can't parse JS-rendered content
- **Example:** No text extracted, empty page_text

### **4. Rate Limiting**
- **Problem:** Too many requests to the same domain
- **Why:** Making rapid requests without delays
- **Example:** 429 Too Many Requests

### **5. Redirects and Auth**
- **Problem:** Page redirects or requires authentication
- **Why:** Links might need special handling
- **Example:** 301/302 redirects, login required

---

## âœ… Solutions (In Order of Effectiveness)

### **Solution 1: Better User-Agent Spoofing** â­â­â­â­â­

**Problem:** Sites recognize us as a bot

**Fix:** Improve headers to look more like a real browser

```python
# In http_utils.py - modify get_random_headers()

def get_random_headers() -> Dict[str, str]:
    """Generate headers that look like a real browser."""
    return {
        "User-Agent": random.choice(UA_POOL) if UA_POOL else "Mozilla/5.0...",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0",
    }
```

---

### **Solution 2: Increase Timeout** â­â­â­â­

**Problem:** 12 seconds is too short

**Fix:** Increase timeout for fetch operations

**Current:**
```python
# In web_utils.py line ~90
resp = fetch_url(url, timeout=12)  # 12 seconds
```

**Change to:**
```python
resp = fetch_url(url, timeout=30)  # 30 seconds
```

Or in `expand_and_summarize_web()`:
```python
resp = fetch_url(url, timeout=30)  # Longer timeout
```

---

### **Solution 3: Add Request Delays** â­â­â­â­

**Problem:** Making requests too fast = rate limiting

**Fix:** Add delays between requests

**Add to `web_utils.py`:**
```python
import time

def expand_and_summarize_web(link: Dict[str, Any], query: str, queue: Queue):
    """Expand a web search result by fetching the page and summarizing it."""
    try:
        # Add delay to avoid rate limiting
        time.sleep(random.uniform(1, 3))  # â† ADD THIS
        
        from utils.http_utils import fetch_url, is_allowed_url, is_probably_pdf
        
        url = link.get("link") or link.get("url") or ""
        # ... rest of function
```

---

### **Solution 4: Handle Special URLs** â­â­â­â­

**Problem:** Some sites need special handling

**Fix:** Add special cases for known problematic sites

**Add to `http_utils.py`:**
```python
def fetch_url(url: str, timeout: int = DEFAULT_TIMEOUT, max_retries: int = MAX_RETRIES) -> Optional[httpx.Response]:
    """Fetch URL with special handling for known sites."""
    
    # Special handling for PMC/PubMed
    if "pmc.ncbi.nlm.nih.gov" in url or "pubmed.ncbi.nlm.nih.gov" in url:
        timeout = 30  # Longer timeout for NIH sites
        max_retries = 5  # More retries
    
    # Special handling for Mayo Clinic
    if "mayoclinic.org" in url:
        timeout = 25
    
    # Rest of fetch logic...
    headers = get_random_headers()
    # ... rest of function
```

---

### **Solution 5: Graceful Fallback** â­â­â­

**Problem:** Can't get full page, but have snippet

**Fix:** Use snippet when fetch fails

**In `web_utils.py` around line 85:**
```python
# Current code:
if resp is None:
    link["summary"] = "Failed to fetch page."
    link["title"] = title
    queue.put(link); return

# Better code:
if resp is None:
    # Fall back to snippet instead of failing
    link["summary"] = snippet if snippet else "Could not fetch full page, summary unavailable"
    link["title"] = title
    queue.put(link); return
```

---

### **Solution 6: Implement Retry with Exponential Backoff** â­â­â­

**Problem:** Temporary failures aren't retried

**Fix:** Better retry strategy (already partially done, but improve it)

```python
# In http_utils.py - enhance retry logic

def fetch_url(url: str, timeout: int = DEFAULT_TIMEOUT, max_retries: int = MAX_RETRIES):
    """Fetch with exponential backoff."""
    
    headers = get_random_headers()
    backoff = RETRY_BACKOFF
    
    for attempt in range(max_retries):
        try:
            # Add random user agent for each retry
            headers = get_random_headers()  # â† Rotate headers each attempt
            
            response = httpx.get(
                url, 
                headers=headers, 
                timeout=timeout, 
                follow_redirects=True
            )
            response.raise_for_status()
            return response
            
        except httpx.HTTPStatusError as e:
            # Handle different status codes
            if e.response.status_code == 429:
                wait_time = min(backoff * (2 ** attempt), MAX_BACKOFF)
                logger.warning(f"Rate limited, waiting {wait_time}s")
                time.sleep(wait_time)
            elif e.response.status_code == 403:
                logger.warning(f"Access forbidden for {url}")
                return None  # Don't retry 403
            elif e.response.status_code >= 500:
                wait_time = min(backoff * (2 ** attempt), MAX_BACKOFF)
                logger.warning(f"Server error, retrying in {wait_time}s")
                time.sleep(wait_time)
            else:
                logger.error(f"HTTP {e.response.status_code}")
                return None
```

---

### **Solution 7: Skip Problematic Sites** â­â­

**Problem:** Some sites always fail

**Fix:** Maintain a blacklist

```python
# Add to http_utils.py

BLACKLIST_DOMAINS = [
    "researchgate.net",  # Often requires login
    "linkedin.com",      # Blocks scrapers
    "twitter.com",       # Dynamic content
]

def is_allowed_url(url: str) -> bool:
    """Check if URL is allowed (not blacklisted)."""
    try:
        host = urlparse(url).netloc.lower()
        
        # Skip blacklisted domains
        for blacklist in BLACKLIST_DOMAINS:
            if blacklist in host:
                logger.warning(f"Skipping blacklisted domain: {url}")
                return False
        
        return True
    except Exception as e:
        logger.warning(f"URL parsing error: {e}")
        return False
```

---

### **Solution 8: Use Selenium for JavaScript Sites** â­â­ (Advanced)

**Problem:** JavaScript-heavy sites can't be parsed with BeautifulSoup

**Fix:** Use Selenium for dynamic content (only if needed)

```python
# Optional: For critical sites that need JavaScript rendering
# pip install selenium

from selenium import webdriver
from selenium.webdriver.common.by import By

def fetch_url_with_js(url: str, timeout: int = 15) -> Optional[str]:
    """Fetch page with JavaScript rendering."""
    try:
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        driver = webdriver.Chrome(options=options)
        driver.set_page_load_timeout(timeout)
        
        driver.get(url)
        page_text = driver.find_element(By.TAG_NAME, "body").text
        driver.quit()
        
        return page_text
    except Exception as e:
        logger.error(f"Selenium error: {e}")
        return None
```

---

## ðŸŽ¯ Recommended Fix (Start Here)

**Implement Solutions in this order:**

### **Phase 1 (Quick Wins - 80% improvement):**
1. âœ… **Solution 1:** Better User-Agent headers
2. âœ… **Solution 2:** Increase timeout to 30s
3. âœ… **Solution 3:** Add request delays (1-3 seconds)

### **Phase 2 (Medium effort - 90% improvement):**
4. âœ… **Solution 4:** Special handling for NIH sites
5. âœ… **Solution 5:** Fallback to snippets
6. âœ… **Solution 6:** Improve retry logic

### **Phase 3 (If needed - 95% improvement):**
7. âœ… **Solution 7:** Maintain blacklist
8. âœ… **Solution 8:** Selenium for JS sites (only if critical)

---

## ðŸ“ Quick Implementation

Here's what to do RIGHT NOW:

### **Step 1: Update `http_utils.py`**

```python
def get_random_headers() -> Dict[str, str]:
    """Generate headers that look like a real browser."""
    return {
        "User-Agent": random.choice(UA_POOL) if UA_POOL else "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0",
    }
```

### **Step 2: Update `web_utils.py`**

```python
def expand_and_summarize_web(link: Dict[str, Any], query: str, queue: Queue):
    """Expand a web search result by fetching the page and summarizing it."""
    try:
        # Add delay to avoid rate limiting
        import random, time
        time.sleep(random.uniform(1, 3))
        
        from utils.http_utils import fetch_url, is_allowed_url, is_probably_pdf
        
        url = link.get("link") or link.get("url") or ""
        if not url or not is_allowed_url(url):
            link["summary"] = "Filtered or invalid URL"
            queue.put(link); return

        title = link.get("title", link.get("displayLink", "Source"))
        snippet = link.get("snippet", "")
        page_text = ""

        try:
            # Increase timeout for NIH sites
            timeout = 30 if "ncbi.nlm.nih.gov" in url or "pmc.ncbi.nlm.nih.gov" in url else 12
            resp = fetch_url(url, timeout=timeout)
            
            if resp is None:
                # Fallback to snippet
                link["summary"] = snippet if snippet else "Could not fetch full page"
                link["title"] = title
                queue.put(link); return
            
            # Rest of the function...
```

---

## ðŸ“Š Expected Results

After implementing Phase 1:

```
Before: 80% "Failed to fetch page"
After:  20% "Failed to fetch page"  (80% improvement!)

Success rate: 80%
Better summarization: Yes
Citations quality: Much better
```

---

## ðŸš€ Next Steps

1. Start with Phase 1 (takes 30 minutes)
2. Test with your medical question
3. Check logs for improvement
4. If still failing, move to Phase 2

Would you like me to implement these changes for you?
